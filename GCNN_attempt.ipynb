{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell imports the necessary packages and loads the ESOL dataset using a graph convolutional featurizer before splitting datasets into their respective datasets\n",
    "#tutorials on DC github (in particular tutorial 06) were referenced heavily\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import deepchem as dc\n",
    "\n",
    "tasks, datasets, transformers = dc.molnet.load_delaney(featurizer='GraphConv')\n",
    "train_dataset, valid_dataset, test_dataset = datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02226135492324829"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this cell creates the model and trains it. 100 rounds of backpropoagation occurred with no early callback adopted (meaning the chances of overfitting are significantly higher)\n",
    "#note that this approach does not allow one to influence the size of the NN nor the shape which are parameters that could be explored in order to reduce the loss function\n",
    "n_tasks = len(tasks)\n",
    "model = dc.models.GraphConvModel(n_tasks, mode='regression')\n",
    "model.fit(train_dataset, nb_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: {'mae_score': 0.2609849997834114}\n",
      "Test set score: {'mae_score': 1.0435558278876267}\n"
     ]
    }
   ],
   "source": [
    "#first metric recorded\n",
    "metric = dc.metrics.Metric(dc.metrics.mae_score)\n",
    "print('Training set score:', model.evaluate(train_dataset, [metric], transformers))\n",
    "print('Test set score:', model.evaluate(test_dataset, [metric], transformers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: {'rms_score': 0.34640199888894085}\n",
      "Test set score: {'rms_score': 1.3309637001470798}\n"
     ]
    }
   ],
   "source": [
    "#second metric recorded\n",
    "metric = dc.metrics.Metric(dc.metrics.rms_score)\n",
    "print('Training set score:', model.evaluate(train_dataset, [metric], transformers))\n",
    "print('Test set score:', model.evaluate(test_dataset, [metric], transformers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: {'pearson_r2_score': 0.9750148657449388}\n",
      "Test set score: {'pearson_r2_score': 0.6298494902434739}\n"
     ]
    }
   ],
   "source": [
    "#third metric recorded\n",
    "metric = dc.metrics.Metric(dc.metrics.pearson_r2_score)\n",
    "print('Training set score:', model.evaluate(train_dataset, [metric], transformers))\n",
    "print('Test set score:', model.evaluate(test_dataset, [metric], transformers))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alfenv] *",
   "language": "python",
   "name": "conda-env-alfenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
